{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face and Facial Keypoint detection\n",
    "\n",
    "\n",
    "1. Detect all the faces in an image using a face detector (we'll be using a Haar Cascade detector in this notebook).\n",
    "2. Pre-process those face images so that they are grayscale, and transformed to a Tensor of the input size that your net expects. This step will be similar to the `data_transform` you created and applied in Notebook 2, whose job was tp rescale, normalize, and turn any iimage into a Tensor to be accepted as input to your CNN.\n",
    "3. Use your trained model to detect facial keypoints on the image.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select an image \n",
    "\n",
    "Select an image to perform facial keypoint detection on; you can select any image of faces in the `images/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "# load in color image for face detection\n",
    "image = cv2.imread('images/obamas.jpg')\n",
    "\n",
    "# switch red and blue color channels \n",
    "# --> by default OpenCV assumes BLUE comes first, not RED as in many images\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# plot the image\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in a haar cascade classifier for detecting frontal faces\n",
    "face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# run the detector\n",
    "# the output here is an array of detections; the corners of each detection box\n",
    "# if necessary, modify these parameters until you successfully identify every face in a given image\n",
    "faces = face_cascade.detectMultiScale(image, 1.2, 2)\n",
    "\n",
    "# make a copy of the original image to plot detections on\n",
    "image_with_detections = image.copy()\n",
    "\n",
    "# loop over the detected faces, mark the image where each face is found\n",
    "for (x,y,w,h) in faces:\n",
    "    # draw a rectangle around each detected face\n",
    "    # you may also need to change the width of the rectangle drawn depending on image resolution\n",
    "    cv2.rectangle(image_with_detections,(x,y),(x+w,y+h),(255,0,0),3) \n",
    "\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "\n",
    "plt.imshow(image_with_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in a trained model\n",
    "\n",
    "Once you have an image to work with (and, again, you can select any image of faces in the `images/` directory), the next step is to pre-process that image and feed it into your CNN facial keypoint detector.\n",
    "\n",
    "First, load your best model by its filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import Net\n",
    "\n",
    "net = Net()\n",
    "\n",
    "\n",
    "net.load_state_dict(torch.load('saved_models/keypoints_model_1.pt'))\n",
    "\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoint detection\n",
    "\n",
    "Now, we'll loop over each detected face in an image (again!) only this time, you'll transform those faces in Tensors that your CNN can accept as input images.\n",
    "\n",
    "### TODO: Transform each detected face into an input Tensor\n",
    "\n",
    "You'll need to perform the following steps for each detected face:\n",
    "1. Convert the face from RGB to grayscale\n",
    "2. Normalize the grayscale image so that its color range falls in [0,1] instead of [0,255]\n",
    "3. Rescale the detected face to be the expected square size for your CNN (224x224, suggested)\n",
    "4. Reshape the numpy image into a torch image.\n",
    "\n",
    "You may find it useful to consult to transformation code in `data_load.py` to help you perform these processing steps.\n",
    "\n",
    "\n",
    "### TODO: Detect and display the predicted keypoints\n",
    "\n",
    "After each face has been appropriately converted into an input Tensor for your network to see as input, you'll wrap that Tensor in a Variable() and can apply your `net` to each face. The ouput should be the predicted the facial keypoints. These keypoints will need to be \"un-normalized\" for display, and you may find it helpful to write a helper function like `show_keypoints`. You should end up with an image like the following with facial keypoints that closely match the facial features on each individual face:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from data_load import ToTensor\n",
    "image_copy = np.copy(image)\n",
    "\n",
    "# loop over the detected faces from your haar cascade\n",
    "for i in range(len(faces)):\n",
    "    (x,y,w,h)  = faces[i]\n",
    "    x = int(0.85*x)\n",
    "    y = int(0.85*y)\n",
    "    h = int(1.5*h)\n",
    "    w = int(1.5*w)\n",
    "    # Select the region of interest that is the face in the image \n",
    "    roi = image_copy[y:y+h, x:x+w]\n",
    "    \n",
    "    ## TODO: Convert the face region from RGB to grayscale\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_RGB2GRAY)\n",
    "    ## TODO: Normalize the grayscale image so that its color range falls in [0,1] instead of [0,255]\n",
    "    roi = roi/255.0\n",
    "    ## TODO: Rescale the detected face to be the expected square size for your CNN (224x224, suggested)\n",
    "    roi = cv2.resize(roi, (224-60, 224-60))\n",
    "    roi = cv2.copyMakeBorder(roi, 30, 30, 30, 30, cv2.BORDER_REPLICATE, value = 0)\n",
    "    img = np.copy(roi)\n",
    "    #plt.subplot(2, 1, i+1)\n",
    "    ## TODO: Reshape the numpy image shape (H x W x C) into a torch image shape (C x H x W)\n",
    "    #plt.imshow(roi, cmap = 'gray')\n",
    "    if(len(roi.shape) == 2):\n",
    "    # add that third color dim\n",
    "        roi = roi.reshape(roi.shape[0], roi.shape[1], 1, 1)\n",
    "            \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        roi = roi.transpose((2, 3, 0, 1))\n",
    "        roi = torch.from_numpy(roi)\n",
    "    \n",
    "    ## TODO: Make facial keypoint predictions using your loaded, trained network \n",
    "    roi = Variable(roi)\n",
    "    ## wrap each face region in a Variable and perform a forward pass to get the predicted facial keypoints\n",
    "    roi = roi.type(torch.FloatTensor)\n",
    "    ## TODO: Display each detected face and the corresponding keypoints        \n",
    "    output_pts = net(roi)\n",
    "    output_pts = output_pts.view(output_pts.size()[0], 68, -1)\n",
    "    output_pts = output_pts.data.numpy()\n",
    "    #print(output_pts.shape)\n",
    "    \n",
    "        # undo normalization of keypoints  \n",
    "    output_pts = output_pts[0]*50.0+100\n",
    "    ## TODO: Reshape the numpy image shape (H x W x C) into a torch image shape (C x H x W)\n",
    "    plt.subplot(2, 1, i+1)\n",
    "    ## TODO: Reshape the numpy image shape (H x W x C) into a torch image shape (C x H x W)\n",
    "    #plt.imshow(roi, cmap = 'gray')\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.scatter(output_pts[:, 0], output_pts[:, 1], s=20, marker='.', c='m')  \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
